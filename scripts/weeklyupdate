#!/usr/bin/env bash
# Jobs that need running weekly
#set -x

source consts

source ../../shlib/deployfns
read_conf "../conf/general"

INDEX="`pwd`/all-titles-in-ns0"

# Get new wikipedia titles database
rm -f "$INDEX.gz" $INDEX

DUMPDATE=`curl -q -o - https://dumps.wikimedia.org/backup-index.html | grep "enwiki/" | perl -pi.bak -e "s/.*(\d\d\d\d\d\d\d\d).*/\\\$1/;"`
#echo "Wikipedia dump date $DUMPDATE"
curl -q -o "$INDEX.gz" https://dumps.wikimedia.org/enwiki/$DUMPDATE/enwiki-$DUMPDATE-all-titles-in-ns0.gz
gunzip "$INDEX.gz"
MYSQL="mysql -u $DB_USER --password=$DB_PASSWORD $DB_NAME"
echo "load data local infile '$INDEX' ignore into table titles;" | $MYSQL
cat wikipedia-exceptions | $MYSQL
# Cleanup
rm -f $INDEX

# Compact Xapian database
./compactsearchdb


#Full database:
#https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2

